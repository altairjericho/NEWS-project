{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-node 20 in parallel mode for Gauss-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "from skimage import img_as_float\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import precision_score, make_scorer, precision_recall_curve, roc_curve, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Set of functions to fit an image with a gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaus_moments(data):\n",
    "    \"\"\"Returns a dict containing initial guesses for 2D-Gaussian parameters: \n",
    "    (background, amplitude, x_mean, y_mean, sigma_x, sigma_y)\n",
    "    \n",
    "    Assuming that image is approximately centered\"\"\"\n",
    "    total = data.sum()\n",
    "    Y, X = np.indices(data.shape)\n",
    "    x = data.shape[0]/2 #(X*data).sum()/total\n",
    "    y = data.shape[1]/2 #(Y*data).sum()/total\n",
    "    col = data[:, int(y)]\n",
    "    sigma_y = np.abs((np.arange(col.size)-y)**2*col).sum()/col.sum()\n",
    "    row = data[int(x), :]\n",
    "    sigma_x = np.abs((np.arange(row.size)-x)**2*row).sum()/row.sum()\n",
    "    height = data.min()\n",
    "    amplitude = data.max() - height\n",
    "    mom={'background':height, 'amplitude':amplitude, 'x_mean':x, 'y_mean':y, 'sigma_x':sigma_x, 'sigma_y':sigma_y}\n",
    "    return mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twod_gaussian(height, amplitude, center_x, center_y, sig_x, sig_y, sig_xy):\n",
    "    \"\"\"Returns a gaussian function with the given parameters\"\"\"\n",
    "    #Sigma = [[sig_x, sig_xy], [sig_xy, sig_y]]\n",
    "    Sigmin= [[sig_y, -sig_xy], [-sig_xy, sig_x]]/(sig_x*sig_y - sig_xy**2 + 10**-4)\n",
    "    center = np.array([center_x, center_y]).reshape((1,1,2))\n",
    "    #print center.shape, Sigmin.shape, center.dot(Sigmin).shape\n",
    "    return lambda x: height+ amplitude*np.exp(-( (x-center).dot(Sigmin)*(x-center) ).sum(axis=-1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gaussian(data):\n",
    "    \"\"\"Returns 2D-Gaussian fit parameters:\n",
    "    (height, amplitude, center_x, center_y, sig_x, sig_y, sig_xy)\n",
    "    found by a optimizing least squares of an error\"\"\"\n",
    "    mome = gaus_moments(data)\n",
    "    para = [mome['background'], mome['amplitude'], mome['x_mean'], mome['y_mean'], mome['sigma_x'], mome['sigma_y'], 1.2]\n",
    "    x = np.stack(np.meshgrid(np.arange(data.shape[0]), np.arange(data.shape[1])), axis=-1)\n",
    "    errorfunction = lambda p: np.ravel(twod_gaussian(*p)(x) - data)\n",
    "    p, success = sp.optimize.leastsq(errorfunction, para)\n",
    "    #print success\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 0.2 Set of functions for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs(dir_name='dust_fog', numb = 10):\n",
    "    \"\"\"\n",
    "    Loading 'numb' of images from 'dir_name'\n",
    "    and reshaping each one into 8 polarization images of shape 30x30\n",
    "    -------\n",
    "    Output:   numpy array of shape 'numb' x 8 x 30 x 30\n",
    "    \"\"\"\n",
    "    dir_name = 'D:/NEWSdm/crops/'+dir_name\n",
    "    img_arr = []\n",
    "    num_i = 0\n",
    "    for img_name in os.listdir(dir_name):\n",
    "        num_i += 1\n",
    "        if num_i > numb :\n",
    "            break\n",
    "        tmp_im = (pd.read_csv(os.path.join(dir_name, img_name), header=None).drop(30, axis=1)).values.reshape((8,30,30))\n",
    "        img_arr.append(tmp_im)\n",
    "    return np.array(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_params(imgs):\n",
    "    \"\"\"\n",
    "    Takes array (shape=[numb,8,30,30]) of numb x 8 images of shape 30 x 30\n",
    "    and fits a 2D Gaussian on each of the images.\n",
    "    -------\n",
    "    Output: DataFrame with fitting parameters for images in the array.\n",
    "    \"\"\"\n",
    "    params = pd.DataFrame(columns=['img_num', 'polar', 'height', 'amplitude', 'center_x', 'center_y', 'sig_x', 'sig_y', 'sig_xy'])\n",
    "    for num in xrange(imgs.shape[0]):\n",
    "        i=0\n",
    "        for data in imgs[num]:\n",
    "            par = np.concatenate(([num+1, i], fit_gaussian(data)))\n",
    "            par = par.reshape((1,len(par)))\n",
    "            i += 1\n",
    "            params = params.append(pd.DataFrame(par,columns=['img_num', 'polar', 'height', 'amplitude', 'center_x', 'center_y', 'sig_x', 'sig_y', 'sig_xy']), ignore_index=True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_params(params, up_bound=500):\n",
    "    \"\"\"\n",
    "    Dropping divergent samples (rows) from the dataset.\n",
    "    Sample is divergent if any parameter (except 'sig_xy') is negative or larger than 'up_bound'\n",
    "    \"\"\"\n",
    "    img_num = params.get('img_num')\n",
    "    cov_xy = params.get('sig_xy')\n",
    "    params = params.drop(['img_num','sig_xy'],axis=1)\n",
    "    params = params[params[params<up_bound]>=0]\n",
    "    params = pd.concat([img_num, params, cov_xy], axis=1, join='inner')\n",
    "    return params.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_features(params):\n",
    "    \"\"\"\n",
    "    Getting physical features from fitting parameters:\n",
    "    polarization number, amplitude, center position, rotation (represented by 'sig_xy'), minor axis, eccentrisity, area\n",
    "    -------\n",
    "    Output: DataFrame with features for every image\n",
    "    \"\"\"\n",
    "    Sigms = params.get(['sig_x','sig_y','sig_xy']).values\n",
    "    idx = params.index\n",
    "    #print Sigms.shape\n",
    "    a_ar = []\n",
    "    area_ar = []\n",
    "    eps_ar = []\n",
    "    for s in Sigms:\n",
    "        #print s\n",
    "        a, b = np.linalg.eigvalsh([[s[0],s[2]],[s[2],s[1]]])\n",
    "        if a>b:\n",
    "            tmp = a\n",
    "            a = b\n",
    "            b = tmp\n",
    "        a_ar.append(a)\n",
    "        area_ar.append(np.pi * a * b)\n",
    "        eps = b/a\n",
    "        if eps<1: eps = 1/eps\n",
    "        eps_ar.append(eps)\n",
    "    ellip_par = pd.DataFrame(data={'minor_ax':a_ar, 'eps':eps_ar, 'area':area_ar}, index=idx)\n",
    "    #print ellip_par.shape\n",
    "    feat = params.drop(['img_num','height','sig_x','sig_y'],axis=1)\n",
    "    feat = pd.concat([feat, ellip_par], axis=1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_to_input(ft_data, cl_names):\n",
    "    \"\"\"\n",
    "    Stacking data with names from 'cl_names' into one DataFrame labeling 'signal' with 1 and 'noise' with 0.\n",
    "    \"\"\"\n",
    "    inp_data = pd.DataFrame(columns=np.concatenate((['target'], ft_data[cl_names[0]].columns.values)) )\n",
    "    for name in cl_names:\n",
    "        idx = ft_data[name].index\n",
    "        if name[0] == 'C' :\n",
    "            targ = pd.DataFrame(data={'target':np.ones(len(idx))}, index=idx)\n",
    "        else: targ = pd.DataFrame(data={'target': np.zeros(len(idx))}, index=idx)\n",
    "        app_data = pd.concat([targ, ft_data[name]], axis=1)\n",
    "        inp_data = inp_data.append(app_data, ignore_index=True)\n",
    "    return inp_data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.1 Special functions for large data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be improved\n",
    "\n",
    "def load_and_fit_batches(class_name='dust_fog', numb = 10000, node=1, n_cpu=1, n_0 = 1000, fit_all = False):\n",
    "    \"\"\"\n",
    "    Loading 'numb' of samples in total from 'class_name' and fitting them with Gaussians and writing results files.\n",
    "    1. Taking 'n_0' samples at a time and rewriting the same variables on each iteration.\n",
    "    2. Pseudo-parallelised by running 'n_cpu' applications at once by hands with different 'node' numbers.\n",
    "    -------\n",
    "    Output:\n",
    "    Written 'csv' files with fitting parameters in specific folders, returns 'nump' totall number of samples processed.\n",
    "    The resulting files are like 'C:/Artem/NEWS/params/batches/50000/gamma/cpu3_n1_it1_params.csv'\n",
    "    \n",
    "    *Important*: 'numb' must be devidible by 'n_0', otherwise up to 'n_0' extra samples wil be processe.\n",
    "    \"\"\"\n",
    "    #Getting the number of iterations on this node.\n",
    "    i_m = np.int(float(numb)/(n_0 * n_cpu))\n",
    "    over = np.ceil((float(numb) - i_m*n_0*n_cpu)/n_0)\n",
    "    if node <= over:\n",
    "        i_m +=1 # each node has it's own i_m\n",
    "        \n",
    "    \n",
    "    dir_name = 'C:/Artem/NEWS/crops/'+class_name\n",
    "    nump = 0\n",
    "    img_names = os.listdir(dir_name)\n",
    "    for i in range(i_m):\n",
    "        #iterating over samples in the batch\n",
    "        img_arr = []\n",
    "        for img_name in img_names[(node+i-1)*n_0 : (node+i)*n_0]:\n",
    "            tmp_im = (pd.read_csv(os.path.join(dir_name, img_name), header=None).drop(30, axis=1)).values.reshape((8,30,30))\n",
    "            img_arr.append(tmp_im)\n",
    "        img_arr = np.array(img_arr)\n",
    "        #getting parametrs of samples in the batch and writing tham to file.\n",
    "        params = pd.DataFrame(columns=['img_num', 'polar', 'height', 'amplitude', 'center_x', 'center_y', 'sig_x', 'sig_y', 'sig_xy'])\n",
    "        for img_num in np.arange(img_arr.shape[0]):\n",
    "            polar=1\n",
    "            for data in img_arr[img_num]:\n",
    "                par = np.concatenate(([img_num+1, polar], fit_gaussian(data)))\n",
    "                par = par.reshape((1,len(par)))\n",
    "                polar += 1\n",
    "                nump += 1\n",
    "                params = params.append(pd.DataFrame(par,columns=['img_num', 'polar', 'height', 'amplitude', 'center_x', 'center_y', 'sig_x', 'sig_y', 'sig_xy']), ignore_index=True)\n",
    "        output_name = str(numb)+'/'+class_name+'/cpu'+str(n_cpu)+'_n'+str(node)+'_it'+str(i+1)+'_params'\n",
    "        if fit_all: output_name = 'ALL/'+class_name+'/cpu'+str(n_cpu)+'_n'+str(node)+'_it'+str(i+1)+'_params'\n",
    "        params.to_csv('C:/Artem/NEWS/params/batches/'+output_name+'.csv')\n",
    "    '''\n",
    "    Пофиксить что самая последняя итерация выбегает за пределы, если не делится нацело.\n",
    "    '''\n",
    "    return nump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_params_from_names(numb=10000, name='dust_fog', fit_all=False):\n",
    "    \"\"\"\n",
    "    Reading and stacking DataFrames with parameters from the specified directory: \n",
    "    C:/Artem/NEWS/params/batches/'numb'/name\n",
    "    \"\"\"\n",
    "    dir_name = 'C:/Artem/NEWS/params/batches/'+str(numb)+'/'+name\n",
    "    if fit_all: dir_name = 'C:/Artem/NEWS/params/batches/ALL/'+name\n",
    "    params = pd.DataFrame(columns=['img_num', 'polar', 'height', 'amplitude', 'center_x', 'center_y', 'sig_x', 'sig_y', 'sig_xy'])\n",
    "    for img_name in os.listdir(dir_name):\n",
    "        par = pd.read_csv(os.path.join(dir_name, img_name), index_col=0)\n",
    "        params = params.append(par, ignore_index=True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params_to_feat(numb, cl_names, giant=False):\n",
    "    \"\"\"\n",
    "    Reading parameters from files, cleaning them and transforming into features.\n",
    "    -------\n",
    "    Output: Dict with features DataFrame for every class name\n",
    "    \"\"\"\n",
    "    dat_params = {}\n",
    "    ft_data = {}\n",
    "    for name in cl_names:\n",
    "        if giant:\n",
    "            dat_params[name] = read_params_from_names(numb, name)\n",
    "        else: dat_params[name] = pd.read_csv('C:/Artem/NEWS/params/'+str(numb)+'_'+name+'_params.csv', index_col=0)\n",
    "        ft_data[name] = params_to_features( clean_params(dat_params[name]) )\n",
    "    return ft_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_to_input_split(ft_data, cl_names, test_size=0.1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splitting data for train-test with or without shuffle.\n",
    "    Stacking specified classes together considering target label (signal/background).\n",
    "    -------\n",
    "    Output: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        inp_data = feat_to_input(ft_data, cl_names)\n",
    "        X = inp_data.drop(['target'], axis=1).values\n",
    "        y = inp_data.get('target').values\n",
    "        X_tr, X_tt, y_tr, y_tt = train_test_split(X, y, test_size=test_size)\n",
    "    else:\n",
    "        y_tr = pd.DataFrame().values\n",
    "        y_tt = pd.DataFrame().values\n",
    "        X_tr = pd.DataFrame(columns=ft_data[cl_names[0]].columns).values\n",
    "        X_tt = pd.DataFrame(columns=ft_data[cl_names[0]].columns).values\n",
    "        for name in cl_names:\n",
    "            if name[0]=='C': \n",
    "                y = np.ones(ft_data[name].shape[0])\n",
    "            else: y = np.zeros(ft_data[name].shape[0])\n",
    "            X = ft_data[name].values\n",
    "            tmp_X_tr, tmp_X_tt, tmp_y_tr, tmp_y_tt = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "            X_tr = np.vstack((X_tr, tmp_X_tr))\n",
    "            X_tt = np.vstack((X_tt, tmp_X_tt))\n",
    "            y_tr = np.append(y_tr, tmp_y_tr)\n",
    "            y_tt = np.append(y_tt, tmp_y_tt)\n",
    "            \n",
    "    return X_tr, X_tt, y_tr, y_tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.2 Functions for all polarisations stacked together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_stacked_polar(params, n_pol=8):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    pol_par = pd.DataFrame()\n",
    "    for i in range(params.shape[0]//n_pol):\n",
    "        par_slice = params[(i*8):((i+1)*8)]\n",
    "        par_slice = pd.DataFrame(par_slice.drop(['img_num','polar'],axis=1).values, index=par_slice.get('polar'), columns=par_slice.drop(['img_num','polar'],axis=1).columns)\n",
    "        #print par_slice.head(2), '\\n'\n",
    "        flat_slice = pd.DataFrame(index=[i])\n",
    "        for j in np.arange(1,n_pol+1):\n",
    "            col_slice = np.append(flat_slice.columns.values, [opa+str(int(j)) for opa in par_slice.loc[j].index.values])\n",
    "            #print col_slice\n",
    "            flat_slice = pd.DataFrame(np.append(flat_slice.values, par_slice.loc[j].values).reshape((1,-1)), columns=col_slice, index=[i])\n",
    "        pol_par = pd.concat([pol_par, flat_slice])\n",
    "    return pol_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stacked_params(params, up_bound=500, n_pol=8):\n",
    "    \"\"\"\n",
    "    Dropping divergent samples (rows) from the dataset.\n",
    "    Sample is divergent if any parameter (except 'sig_xy') is negative or larger than 'up_bound'\n",
    "    \"\"\"\n",
    "    img_num = params.index\n",
    "    for par in params.columns:\n",
    "        if par[:6] != 'sig_xy':\n",
    "            params = params[ params[par]>=0 ]\n",
    "        params = params[ params[par]<up_bound ]\n",
    "    print (params.shape)\n",
    "    return params.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_feat_stacked(params, n_pol=8):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    idx = params.index\n",
    "    feat = params.copy()\n",
    "    for i in np.arange(1,n_pol+1):\n",
    "        Sigms = params.get(['sig_x'+str(i),'sig_y'+str(i),'sig_xy'+str(i)]).values\n",
    "        a_ar = []\n",
    "        area_ar = []\n",
    "        eps_ar = []\n",
    "        b_ar = []\n",
    "        for s in Sigms:\n",
    "            a, b = np.linalg.eigvalsh([[s[0],s[2]],[s[2],s[1]]])\n",
    "            if a>b:\n",
    "                tmp = a\n",
    "                a = b\n",
    "                b = tmp\n",
    "            a_ar.append(a)\n",
    "            area_ar.append(np.pi * a * b)\n",
    "            b_ar.append(b)\n",
    "            eps_ar.append(b/a)\n",
    "        ellip_par = pd.DataFrame(data={'min_ax'+str(i):a_ar, 'maj_ax'+str(i):b_ar, 'eps'+str(i):eps_ar, 'area'+str(i):area_ar}, index=idx)\n",
    "        feat = feat.drop(['height'+str(i),'sig_x'+str(i),'sig_y'+str(i)],axis=1)\n",
    "        feat = pd.concat([feat, ellip_par], axis=1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['C60keV','C80keV','C100keV','dust_fog','gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fitting on ALL samples of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Fitting the images and saving parameters into files\n",
    "* _repeated_ is for recreating the output directory if you are running the same number of samples not for the first time\n",
    "* 20000 images on two nodes ~14hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Artem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n",
      "C:\\Artem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in multiply\n",
      "  import sys\n",
      "C:\\Artem\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\minpack.py:427: RuntimeWarning: Number of calls to function has reached maxfev = 1600.\n",
      "  warnings.warn(errors[info][0], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C60keV': 48000, 'C80keV': 64000, 'C100keV': 64000, 'dust_fog': 64000, 'gamma': 64000}\n",
      "Wall time: 4h 20min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "numb = {}\n",
    "numb['C60keV'] = 123000\n",
    "numb['C80keV'] = 171000\n",
    "numb['C100keV'] = 161000\n",
    "numb['dust_fog'] = 173000\n",
    "numb['gamma'] = 164000\n",
    "giant_data_nump = {}\n",
    "for name in class_names:\n",
    "    giant_data_nump[name] = load_and_fit_batches(class_name=name, numb=numb[name], node=20, n_cpu=20, fit_all=True)\n",
    "print (giant_data_nump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 51. TESTING AREA\n",
    "\n",
    "##### <font color='red'> RANDOM CODE HAPPENS </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
